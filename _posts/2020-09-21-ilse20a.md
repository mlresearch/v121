---
title: 'DIVA: Domain Invariant Variational Autoencoders'
abstract: We consider the problem of domain generalization, namely, how to learn representations
  given data from a set of domains that generalize to data from a previously unseen
  domain. We propose the Domain Invariant Variational Autoencoder (DIVA), a generative
  model that tackles this problem by learning three independent latent subspaces,
  one for the domain, one for the class, and one for any residual variations. We highlight
  that due to the generative nature of our model we can also incorporate unlabeled
  data from known or previously unseen domains. To the best of our knowledge this
  has not been done before in a domain generalization setting. This property is highly
  desirable in fields like medical imaging where labeled data is scarce. We experimentally
  evaluate our model on the rotated MNIST benchmark and a malaria cell images dataset
  where we show that (i) the learned subspaces are indeed complementary to each other,
  (ii) we improve upon recent works on this task and (iii) incorporating unlabelled
  data can boost the performance even further.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ilse20a
month: 0
tex_title: 'DIVA: Domain Invariant Variational Autoencoders'
firstpage: 322
lastpage: 348
page: 322-348
order: 322
cycles: false
bibtex_author: Ilse, Maximilian and Tomczak, Jakub M. and Louizos, Christos and Welling,
  Max
author:
- given: Maximilian
  family: Ilse
- given: Jakub M.
  family: Tomczak
- given: Christos
  family: Louizos
- given: Max
  family: Welling
date: 2020-09-21
address: 
container-title: Proceedings of the Third Conference on Medical Imaging with Deep
  Learning
volume: '121'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 9
  - 21
pdf: http://proceedings.mlr.press/v121/ilse20a/ilse20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
